<!DOCTYPE html>
<html>
	<head>
		<title>Gyuhak Kim</title>
		<!-- link to main stylesheet -->
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<link rel="stylesheet" type="text/css" href="/css/main.css">
		<script src="myscripts.js"></script>
	</head>
	<body>
		<nav>
			<ul>
        		<li><a href="mailto:gkim87@uic.edu">Email</a></li>
        		<li><a href="https://github.com/k-gyuhak">Github</a></li>
				<li><a href="https://scholar.google.com/citations?hl=en&user=EguoUcYAAAAJ">Google Scholar</a></li>
				<li><a href="https://twitter.com/gyuhak_">Twitter</a></li>
				<li><a href="https://www.linkedin.com/in/gyuhak/">LinkedIn</a></li>
			</ul>
		
		</nav>
		<h2>Gyuhak Kim</h2>
		<div class="container">
    		<div class="blurb">
        		<h3></h3>
				<!-- <img src = "IMG_3822.jpeg" height="750" width="1000" object-fit="cover"  style="vertical-align:middle;margin:0px 50px"> -->
				<img src = "IMG_3822.jpeg" height="231.3" width="177.9" object-fit="cover" style="float:left;padding: 10px" CLEAR=ALL>
				<p>
					I am a PhD student at University of Illnois at Chicago (UIC). My advisor is 
					<a href='https://www.cs.uic.edu/~liub/'>
						<font color='black'>
						<u>Bing Liu</u>.
						</font>
					</a>
					I try to build an autonomous system that is capable of detecting unknown instances (i.e., out-of-distribution detection) and learning a new knowledge continually (i.e., continual learning). Prior to studying at UIC, I obtained BA in economics and statistics (double major) from Washington University in Saint Louis (WashU), and obtained MS in mathematics from New York University (NYU).
				</p>
    		</div><!-- /.blurb -->
			<!-- <clear> -->
			<p style="clear:both;">

    		<div class="blurb">	
			<h3> Papers </h3>
			<dl>
				<dt>
					<span>&#8226;</span>
					A Theoretical Study on Solving Continual Learning.
					<b>Gyuhak Kim</b><sup>*</sup>, Changnan Xiao<sup>*</sup>, Tatsuya Konishi, Zixuan Ke, Bing Liu, (* equal contribution).
					NeurIPS 2022. <a href="https://arxiv.org/pdf/2211.02633v1.pdf"><font color="black"><u>link</u></font></a>
					<button onclick="myFunction('myDIV')">tl;dr</button>
					<div id="myDIV" style="display:none">
						<FONT COLOR="blue">
							We provide a theoretical guidance on how to solve CIL. It connects CIL, TIL, and OOD. The theoretical analysis is agnostic to specific implementations and is applicable to any CIL problem settings (e.g., blurry, online, offline, etc.). Based on the analysis, we design several CIL methods and show superior performances to the sota baselines.
						</FONT>
					</div>
				</dt>
				<!-- <br> -->
				
				<dt>
					<span>&#8226;</span>
					A Multi-Head Model for Continual Learning via Out-of-Distribution Replay.
					<b>Gyuhak Kim</b>, Zixuan Ke, Bing Liu.
					CoLLAs 2022.
					<a href="https://arxiv.org/pdf/2208.09734.pdf"><font color="black"><u>link</u></font></a>
					<button onclick="myFunction('myDIV1')">tl;dr</button>
					<div id="myDIV1" style="display:none">
						<FONT COLOR="blue">
							We propose a novel replay-based CIL method. Instead of using the replay samples to prevent forgetting as the existing methods, we use them to train the network to perform an OOD detection. The proposed method is evaluated using a recently proposed transformer architecture. It outperforms the baselines and is effective with little replay samples. Its algorithm design is inspired by our theoretical paper above.
						</FONT>
					</div>
				</dt>
				<!-- <br> -->
				
				<dt>
					<span>&#8226;</span>
					Continual Learning Based on OOD Detection and Task Masking.
					<b>Gyuhak Kim</b>, Sepideh Esmaeilpour, Changnan Xiao, Bing Liu.
					CVPR Workshops 2022.
					<a href="https://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Kim_Continual_Learning_Based_on_OOD_Detection_and_Task_Masking_CVPRW_2022_paper.pdf"><font color="black"><u>link</u></font></a>
					<button onclick="myFunction('myDIV2')">tl;dr</button>
					<div id="myDIV2" style="display:none">
						<FONT COLOR="blue">
							We propose a novel CIL method based on OOD detection and TIL. CIL and TIL methods have their own advantages, but they are developed independently A method intended for CIL is weaker in TIL, and vice versa. The proposed method is superior both in TIL and CIL to strong baselines. We achieve it by leveraging a TIL method with the contrastive learning and building OOD detectors. 
						</FONT>
					</div>
				</dt>
				<!-- <br> -->

				<dt>
					<span>&#8226;</span>
					Partially Relaxed Masks for Knowledge Transfer Without Forgetting in Continual Learning.
					Tatsuya Konishi, Mori Kurokawa, Chihiro Ono, Zixuan Ke, <b>Gyuhak Kim</b>, Bing Liu.
					PAKDD 2022.
					<a href="https://link.springer.com/chapter/10.1007/978-3-031-05933-9_29"><font color="black"><u>link</u></font></a>
					<button onclick="myFunction('myDIV3')">tl;dr</button>
					<div id="myDIV3" style="display:none">
						<FONT COLOR="blue">
							We propose a novel TIL method with positive forward transfer. Recent TIL methods suffer little or no forgetting, but are weak in knowledge transfer. We overcome this limitation by relaxing the protected parameters. This allows forward transfer and thus, results in better performances.
						</FONT>
					</div>
				</dt>
				<!-- <br> -->

				<dt>
					<span>&#8226;</span>
					Continual Learning Using Pseudo-Replay via Latent Space Sampling.
					<b>Gyuhak Kim</b>, Sepideh Esmaeilpour, Zixuan Ke, Tatsuya Konishi, Bing Liu.
					Preprint 2021.
					<a href="https://openreview.net/pdf?id=nMo44IjBHX5"><font color="black"><u>link</u></font></a>
					<button onclick="myFunction('myDIV4')">tl;dr</button>
					<div id="myDIV4" style="display:none">
						<FONT COLOR="blue">
							Existing CL methods train the model from scratch despite the fact that strong pre-trained models are available. We propose a simple, efficient, yet effective method based on a pre-trained model. The proposed method generates pseudo feature vectors from the latent space to prevent forgetting. The system outperforms the baselines by large margins.
						</FONT>
					</div>
				</dt>
				<!-- <br> -->

				<dt>
					<span>&#8226;</span>
					Continual Learning via Principal Components Projection.
					<b>Gyuhak Kim</b> and Bing Liu.
					Preprint 2019.
					<a href="https://openreview.net/pdf?id=SkxlElBYDS"><font color="black"><u>link</u></font></a>
					<button onclick="myFunction('myDIV5')">tl;dr</button>
					<div id="myDIV5" style="display:none">
						<FONT COLOR="blue">
							We propose a novel TIL method based on an orthogonal projection of parameters. We project parameters of each task onto a task-specific space orthogonal to the previous tasks. By construction, the system does not suffer from forgetting. To our knowledge, this approach was the first in the familiy of orthogonal weight modification methods in TIL, which now became popular, when we wrote it.
						</FONT>
					</div>
				</dt>
				<!-- <br> -->

			</div><!-- /.blurb -->	
		</div><!-- /.container -->
		<footer>
			<ul>Last Updated: 10/10/2022</ul>
    	</footer>
	</body>
</html>
