<!DOCTYPE html>
<html>
	<head>
		<title>Gyuhak Kim</title>
		<!-- link to main stylesheet -->
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<link rel="stylesheet" type="text/css" href="/css/main.css">
		<script src="myscripts.js"></script>
	</head>
	<body>
		<nav>
			<ul>
        		<li><a href="mailto:gkim87@uic.edu">Email</a></li>
        		<li><a href="https://github.com/k-gyuhak">Github</a></li>
				<li><a href="https://scholar.google.com/citations?hl=en&user=EguoUcYAAAAJ">Google Scholar</a></li>
				<li><a href="https://twitter.com/gyuhak_">Twitter</a></li>
				<li><a href="https://www.linkedin.com/in/gyuhak/">LinkedIn</a></li>
			</ul>
		
		</nav>
		<h2>Gyuhak Kim</h2>
		<div class="container">
    		<div class="blurb">
        		<h3></h3>
				<!-- <img src = "IMG_3822.jpeg" height="750" width="1000" object-fit="cover"  style="vertical-align:middle;margin:0px 50px"> -->
				<img src = "IMG_3822.jpeg" height="231.3" width="177.9" object-fit="cover" style="float:left;padding: 10px" CLEAR=ALL>
				<p>
					I am currently pursuing my PhD degree at the University of Illinois at Chicago (UIC) under the guidance of 
					<a href='https://www.cs.uic.edu/~liub/'>
						<font color='black'>
						<u>Bing Liu</u>.
						</font>
					</a>
					My research work involves the development of an autonomous system with the ability to detect unknown instances (out-of-distribution detection) and continually learn new knowledge (continual learning). Prior to joining UIC, I completed my undergraduate studies in economics and statistics (double major) at Washington University in Saint Louis (WashU) and earned my master's degree in mathematics from New York University (NYU).
				</p>
    		</div><!-- /.blurb -->
			<!-- <clear> -->
			<p style="clear:both;">

    		<div class="blurb">	
			<h3> Papers </h3>
			<dl>
				<dt>
					<span>&#8226;</span>
					Continual Learning: Learnability and Algorithm.
					<b>Gyuhak Kim</b><sup>*</sup>, Changnan Xiao<sup>*</sup>, Tatsuya Konishi, Bing Liu.
					<i>ICML 2023</i>. <a href="https://openreview.net/pdf?id=v2Xv1EbBkH"><font color="black"><u>link</u></font></a>
					<button onclick="myFunction('myDIV10')">tl;dr</button>
					<div id="myDIV10" style="display:none">
						<FONT COLOR="blue">
							In this paper, we show that class-incremental learning (CIL) is in fact learnable. The learnability arises from the fact that OOD detection is learnable (Fang et al., 2022) and CIL problem can be solved by out-of-distribution detection, as demonstrated in our NeurIPS 2022 publication. Our theory also shows that by training a continual learning algorithm with out-of-distribution (OOD) detection capabilities for each task, the problem can be reduced to multi-task learning. Based on the theoretical analysis, we propose a novel CIL method that outperforms the existing state-of-the-art CL methods.
						</FONT>
					</div>
				</dt>
				<!-- <br> -->

				<dt>
					<span>&#8226;</span>
					Parameter-Level Soft-Masking for Continual Learning.
					Tatsuya Konishi, Mori Kurokawa, Chihiro Ono, Zixuan Ke, <b>Gyuhak Kim</b>, Bing Liu.
					<i>ICML 2023</i>. <a href="https://openreview.net/pdf?id=wxFXvPdVqi"><font color="black"><u>link</u></font></a>
					<button onclick="myFunction('myDIV9')">tl;dr</button>
					<div id="myDIV9" style="display:none">
						<FONT COLOR="blue">
							Existing research on task-incremental learning (TIL) primarily focuses on preventing catastrophic forgetting. In this paper, we introduces a novel technique called soft-masking of parameter-level gradient flow (SPG). The proposed method enjoys knowledge transfer while effectively prevents forgetting. We show the effectiveness of the proposed method on both similar and dissimilar tasks.
						</FONT>
					</div>
				</dt>
				<!-- <br> -->

				<dt>
					<span>&#8226;</span>
					Open-World Continual Learning: Unifying Novelty Detection and Continual Learning.
					<b>Gyuhak Kim</b>, Changnan Xiao, Tatsuya Konishi, Zixuan Ke, Bing Liu.
					<i>Preprint 2023</i>. <a href="https://arxiv.org/pdf/2304.10038.pdf"><font color="black"><u>link</u></font></a>
					<button onclick="myFunction('myDIV8')">tl;dr</button>
					<div id="myDIV8" style="display:none">
						<FONT COLOR="blue">
							In order to make an AI agent thrive in the open world, it has to detect novelties and learn them incrementally to make the system more knowledgeable over time. The former is called out-of-distribution (OOD) detection and the latter is called continual learning. In the research community, the two are regarded as two completely different problems. This paper theoretically unifies them by proving that good OOD detection is, in fact, necessary for class-incremental learning (CIL). Thus, a good CIL algorithm can naturally be used in the open world to perform both problems.
						</FONT>
					</div>
				</dt>
				<!-- <br> -->

				<dt>
					<span>&#8226;</span>
					Continual Pre-Training of Language Models.
					Zixuan Ke<sup>*</sup>, Yijia Shao<sup>*</sup>, Haowei Lin<sup>*</sup>, Tatsuya Konishi, <b>Gyuhak Kim</b>, Bing Liu.
					<i>ICLR 2023</i>. <a href="https://arxiv.org/pdf/2302.03241.pdf"><font color="black"><u>link</u></font></a>
					<button onclick="myFunction('myDIV7')">tl;dr</button>
					<div id="myDIV7" style="display:none">
						<FONT COLOR="blue">
							This paper proposes a novel method to continually domain-adaptive pre-train (DAP) a language model (LM) with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their endtask performances. Several techniques are proposed. Extensive experiments show that the method not only overcomes catastrophic forgetting, but also achieves knowledge transfer to improve end-task performances.
						</FONT>
					</div>
				</dt>
				<!-- <br> -->

				<dt>
					<span>&#8226;</span>
					A Theoretical Study on Solving Continual Learning.
					<b>Gyuhak Kim</b><sup>*</sup>, Changnan Xiao<sup>*</sup>, Tatsuya Konishi, Zixuan Ke, Bing Liu.
					<i>NeurIPS 2022</i>. <a href="https://arxiv.org/pdf/2211.02633v1.pdf"><font color="black"><u>link</u></font></a>
					<button onclick="myFunction('myDIV6')">tl;dr</button>
					<div id="myDIV6" style="display:none">
						<FONT COLOR="blue">
							We provide a theoretical guidance on how to solve class-incremental learning (CIL). It connects CIL, task-incremental learning (TIL), and out-of-distribution (OOD) detection. The theoretical analysis is general as it is agnostic to specific implementations and is applicable to any CIL problem settings (e.g., blurry, online, offline, etc.). Based on the analysis, we design several CIL methods and show superior performances to the state-of-the-art baselines.
						</FONT>
					</div>
				</dt>
				<!-- <br> -->
				
				<dt>
					<span>&#8226;</span>
					A Multi-Head Model for Continual Learning via Out-of-Distribution Replay.
					<b>Gyuhak Kim</b>, Zixuan Ke, Bing Liu.
					<i>CoLLAs 2022</i>.
					<a href="https://arxiv.org/pdf/2208.09734.pdf"><font color="black"><u>link</u></font></a>
					<button onclick="myFunction('myDIV5')">tl;dr</button>
					<div id="myDIV5" style="display:none">
						<FONT COLOR="blue">
							We propose a novel replay-based class-incremental learning (CIL) method. Instead of using the replay samples to prevent forgetting as the existing methods, we use them as out-of-distribution (OOD) samples to train the network for OOD detection. The proposed method is evaluated using a recently proposed transformer architecture. It outperforms the baselines and is effective with few replay samples. Its algorithm design is inspired by the theory in our NeurIPS 2022 publication.
						</FONT>
					</div>
				</dt>
				<!-- <br> -->
				
				<dt>
					<span>&#8226;</span>
					Continual Learning Based on OOD Detection and Task Masking.
					<b>Gyuhak Kim</b>, Sepideh Esmaeilpour, Changnan Xiao, Bing Liu.
					<i>CVPR Workshops 2022</i>.
					<a href="https://openaccess.thecvf.com/content/CVPR2022W/CLVision/papers/Kim_Continual_Learning_Based_on_OOD_Detection_and_Task_Masking_CVPRW_2022_paper.pdf"><font color="black"><u>link</u></font></a>
					<button onclick="myFunction('myDIV4')">tl;dr</button>
					<div id="myDIV4" style="display:none">
						<FONT COLOR="blue">
							We propose a novel class-incremental learning (CIL) method based on out-of-distribution (OOD) detection and task-incremental learning (TIL). CIL and TIL methods have their own advantages, and they are developed independently. A continual learning method intended for CIL is weaker in TIL, and vice versa. The proposed method is superior in both TIL and CIL. We achieve it by leveraging a TIL method with contrastive learning and building OOD detectors. 
						</FONT>
					</div>
				</dt>
				<!-- <br> -->

				<dt>
					<span>&#8226;</span>
					Partially Relaxed Masks for Knowledge Transfer Without Forgetting in Continual Learning.
					Tatsuya Konishi, Mori Kurokawa, Chihiro Ono, Zixuan Ke, <b>Gyuhak Kim</b>, Bing Liu.
					<i>PAKDD 2022</i>.
					<a href="https://link.springer.com/chapter/10.1007/978-3-031-05933-9_29"><font color="black"><u>link</u></font></a>
					<button onclick="myFunction('myDIV3')">tl;dr</button>
					<div id="myDIV3" style="display:none">
						<FONT COLOR="blue">
							We propose a novel task-incremental learning (TIL) method with positive forward transfer. Recent TIL methods suffer little or no forgetting, but are weak in knowledge transfer. We overcome this limitation by relaxing the protected parameters. This allows forward transfer and thus, results in better performances.
						</FONT>
					</div>
				</dt>
				<!-- <br> -->

				<dt>
					<span>&#8226;</span>
					Continual Learning Using Pseudo-Replay via Latent Space Sampling.
					<b>Gyuhak Kim</b>, Sepideh Esmaeilpour, Zixuan Ke, Tatsuya Konishi, Bing Liu.
					<i>Preprint 2021</i>.
					<a href="https://openreview.net/pdf?id=nMo44IjBHX5"><font color="black"><u>link</u></font></a>
					<button onclick="myFunction('myDIV2')">tl;dr</button>
					<div id="myDIV2" style="display:none">
						<FONT COLOR="blue">
							Existing continual learning (CL) methods train the model from scratch despite the fact that strong pre-trained models are available. We propose a simple, efficient, yet effective method based on a pre-trained model. The proposed method generates pseudo feature vectors from the latent space to prevent forgetting. The system outperforms the baselines by large margins.
						</FONT>
					</div>
				</dt>
				<!-- <br> -->

				<dt>
					<span>&#8226;</span>
					Continual Learning via Principal Components Projection.
					<b>Gyuhak Kim</b> and Bing Liu.
					<i>Preprint 2019</i>.
					<a href="https://openreview.net/pdf?id=SkxlElBYDS"><font color="black"><u>link</u></font></a>
					<button onclick="myFunction('myDIV1')">tl;dr</button>
					<div id="myDIV1" style="display:none">
						<FONT COLOR="blue">
							We propose a novel task-incremental learning (TIL) method based on an orthogonal projection of parameters. We project the parameters of each task onto a task-specific space orthogonal to the previous tasks. By construction, the system does not suffer from forgetting. To our knowledge, this approach was the first in the family of orthogonal weight modification methods in TIL, which now became popular.
						</FONT>
					</div>
				<br>
			(* equal contribution)
				</dt>
			</div><!-- /.blurb -->	

			<h3> Work Experience </h3>
			<dl>
				<dt>
					<span>&#8226;</span>
					Bosch Research, Audio AI Research Intern. May 2023 - Aug. 2023
<!-- 					<div>
						&ensp; Conference: ICLR, NeurIPS, ACL, EMNLP, COLING, CoLLAs <br>
						&ensp; Journal: TPAMI
					</div> -->
				</dt>

			</div><!-- /.blurb -->


			<h3> Academic Services </h3>
			<dl>
				<dt>
					<span>&#8226;</span>
					Reviewer
					<div>
						&ensp; Conference: ICLR, NeurIPS, ACL, EMNLP, COLING, CoLLAs <br>
						&ensp; Journal: TPAMI
					</div>
				</dt>

			</div><!-- /.blurb -->


			<h3> Presentations and Tutorials </h3>
			<dl>
				<dt>
					<span>&#8226;</span>
					<i>Invited talk</i>. <q>Class Incremental Learning: Learnability and Algorithm.</q> KDDI Symposium, 2023, Remote.
				</dt>
				<dt>
					<span>&#8226;</span>
					<i>Invited tutorial</i>. <q>Recent Advances in Class Incremental Learning.</q> 15th Conference for Data Engineering and Information Management, 2023, Remote.
				</dt>
				<dt>
					<span>&#8226;</span>
					<i>Paper presentation</i>. <q>A Theoretical Study on Solving Continual Learning.</q> In Advances of Neural Information Processing Systems, 2022, Remote.
				</dt>
				<dt>
					<span>&#8226;</span>
					<i>Paper presentation</i>. <q>A Multi-Task Model for Continual Learning via Out-of-Distribution Replay.</q> In Conference of Lifelong Learning Agents, 2022, Remote.
				</dt>

			</div><!-- /.blurb -->





		</div><!-- /.container -->
		<footer>
			<ul>Last Updated: 09/09/2023</ul>
    	</footer>
	</body>
</html>
